---
title: "Geography of fear"
author: "Craig Jolley"
date: "November 5, 2015"
output: html_document
---

For now, I'm basing this on a version of the fear index where I'm removing `vic45` and `a4`. In addition, the following variables were not collected in El Salvador (although they were in both Honduras and Guatemala):

- `vic40`: Out of fear of crime, has limited the places to go shopping
- `vic41`: Out of fear of crime, has limited the places to go for recreation
- `vic43`: Out of fear of crime, has felt the need to change neighborhoods
- `fear6f`: Insecurity at Schools


```{r}
library(ggplot2)
library(mice)
library(plyr)
library(GGally)

lapop.2014.HND <- read.csv("../2014-HND.csv",stringsAsFactors=FALSE)
lapop.2014.SLV <- read.csv("../2014-SLV.csv",stringsAsFactors=FALSE)
lapop.2014.GTM <- read.csv("../2014-GTM.csv",stringsAsFactors=FALSE)
set.seed(12345) # for reproducibility

incl <- c('vic44','fear10','vic1ext',
         'vic1exta','vic1hogar','aoj11','pese1','pese2','aoj17','diso7',
         'diso8','diso10','diso18','diso14','diso16','diso17','vicbar1', 
         'vicbar1f','vicbar3','vicbar4','vicbar7')

hnd_data <- lapop.2014.HND[,incl]
slv_data <- lapop.2014.SLV[,incl]
gtm_data <- lapop.2014.GTM[,incl]
my_data <- rbind(gtm_data,hnd_data,slv_data)

my_data$vic1exta[my_data$vic1exta == 999999] <- 0
is.na(my_data[my_data>30]) <- TRUE

my_imp <- mice(my_data, printFlag = F)
pr <- lapply(1:5,function(x) prcomp(complete(my_imp,x),scale=FALSE,center=TRUE))

all_pc1 <- data.frame(llply(1:5, function(i) pr[[i]]$x[,1]))
names(all_pc1) <- c('imp1', 'imp2', 'imp3', 'imp4', 'imp5')
ggpairs(all_pc1) + theme_classic()
```

This is a bit of a problem -- `imp1` and `imp4` appear to have values that are the negative of all the others. We'll have to flip their signs.

```{r}
all_pc1$imp1 <- -all_pc1$imp1
all_pc1$imp4 <- -all_pc1$imp4
ggpairs(all_pc1) + theme_classic()
```

That's better. Moving on...

```{r}
all_pc1$avg <- rowMeans(all_pc1)
colMeans(my_data[all_pc1$avg < quantile(all_pc1$avg)[2],],na.rm = TRUE)
colMeans(my_data[all_pc1$avg > quantile(all_pc1$avg)[4],], na.rm = TRUE)
```

The values look to be reversed -- higher scores seem to correlate with less fear. Let's reverse things.

```{r}
all_pc1$norm <- scale(-all_pc1$avg) 
predict_data <- data.frame(vic44 = c(0,1), fear10 = c(0,1), 
                           vic1ext = c(2,1),vic1exta=c(0,20),
                           vic1hogar=c(2,1),aoj11=c(1,4),pese1=c(3,1),
                           pese2=c(3,1),aoj17=c(4,1),diso7=c(5,1),diso8=c(5,1),
                           diso10=c(5,1),diso18=c(5,1),diso14=c(5,1),
                           diso16=c(5,1),diso17=c(5,1),vicbar1=c(2,1),
                           vicbar1f=c(3,1),vicbar3=c(2,1),vicbar4=c(2,1),
                           vicbar7=c(2,1))

tmp <- sapply(1:5, function(i) predict(pr[[i]],predict_data)[,1])
# we've got those nasty sign flips in imp1 and imp4 again that we have to fix
tmp[,1] <- -tmp[,1]
tmp[,4] <- -tmp[,4]
minmax <- rowMeans(tmp)
minmax <- -(minmax - mean(all_pc1$avg)) / sd(all_pc1$avg)
# you can check that the values in minmax are really close to the minimum and maximum values you 
# get from quantile(all_pc1$norm)

fear <- data.frame(w=all_pc1$norm) 

predict_data2 <- data.frame(diag(21))
names(predict_data2) <- names(predict_data)
# right now, you have something that would feed 0's and 1's into your score
# prediction -- this doesn't really make sense because many of your questions
# are on a scale from 1-2 or 1-4. Here's one (slightly clunky) way to fix it:
predict_data2$vic1ext <- 2 - predict_data2$vic1ext
predict_data2$vic1hogar <- 2 - predict_data2$vic1hogar
predict_data2$aoj11 <- 1 + 3*predict_data2$aoj11
predict_data2$aoj17 <- 4 - 3*predict_data2$aoj17
predict_data2$pese1 <- 3 - 2*predict_data2$pese1
predict_data2$pese2 <- 3 - 2*predict_data2$pese2
predict_data2$diso7 <- 5 - 4*predict_data2$diso7
predict_data2$diso8 <- 5 - 4*predict_data2$diso8
predict_data2$diso10 <- 5 - 4*predict_data2$diso10
predict_data2$diso18 <- 5 - 4*predict_data2$diso18
predict_data2$diso14 <- 5 - 4*predict_data2$diso14
predict_data2$diso16 <- 5 - 4*predict_data2$diso16
predict_data2$diso17 <- 5 - 4*predict_data2$diso17
predict_data2$vicbar1 <- 2 - predict_data2$vicbar1
predict_data2$vicbar1f <- 3 - 2*predict_data2$vicbar1f
predict_data2$vicbar3 <- 2 - predict_data2$vicbar3
predict_data2$vicbar4 <- 2 - predict_data2$vicbar4
predict_data2$vicbar7 <- 2 - predict_data2$vicbar7



# Now, in each row of predict_data2, all values are those that will give a lower score,
# except for one.
tmp <- sapply(1:5, function(i) predict(pr[[i]],predict_data2)[,1])
tmp[,1] <- -tmp[,1]
tmp[,4] <- -tmp[,4]
scores <- rowMeans(tmp)
scores <- -(scores - mean(all_pc1$avg))/sd(all_pc1$avg)
# Now you can see that all of the scores are reasonably close to the minimum value
# stored in minmax.

diff <- data.frame(d=(scores-minmax[1]), n = names(predict_data))
ggplot(diff, aes(x=n,y=d)) + 
  geom_bar(stat = 'identity', fill = 'skyblue') + 
  coord_flip() + 
  theme_classic() + 
  theme(text = element_text(size=20), 
        axis.title.y = element_blank(),
        axis.title.x = element_blank())
```

Let's check the p-values:

```{r}
lmp <- function (modelobject) {
  if (class(modelobject) != "lm") stop("Not an object of class 'lm' ")
    f <- summary(modelobject)$fstatistic
    p <- pf(f[1],f[2],f[3],lower.tail=F)
    attributes(p) <- NULL
    return(p)
}
lm_data <- my_data[,c('vic1exta','aoj11','aoj17','pese1','pese2',
                  'diso7','diso8','diso10','diso14','diso16','diso17',
                  'diso18','vicbar3')]
sapply(lm_data, function(x) lmp(lm(x ~ all_pc1$norm)))

```

And then for the binary variables:

```{r}
log_data <- my_data[,c('vic1ext','vic1hogar',
                       'vicbar1','vicbar3','vicbar4','vicbar7')]
log_data <- log_data-1
log_data$vic44 <- my_data[,'vic44']
log_data$fear10 <- my_data[,'fear10']

sapply(log_data, function(x) coef(summary(glm(x ~ all_pc1$norm,family=binomial(logit))))[2,4])
```

Finally, let's save ourselves the headache of doing this again:

```{r}
ids <- c(lapop.2014.GTM$idnum,
         lapop.2014.HND$idnum,
         lapop.2014.SLV$idnum)
output <- data.frame(idnum=ids,fear=all_pc1$norm)
write.csv(output,'fear.csv')
```

OK. So now we've got a fear index for all three countries. Now we need to get the relevant geographic information for each.

```{r}
geo <- c('pais','estratopri','estratosec','prov','municipio')
# HND has Distrito information; GTM doesn't
geo_gtm <- lapop.2014.GTM[,geo]
geo_hnd <- lapop.2014.HND[,geo] 
geo_slv <- lapop.2014.SLV[,geo] 

my_geo <- rbind(geo_gtm,geo_hnd,geo_slv)
my_geo$fear <- all_pc1$norm
```

What I want to do now is aggregate by different geographic units and see how much of the variance can be explained by each level of aggregation. First, the country level (`pais`).

```{r}
pais_avg <- ddply(my_geo,~pais,summarize,x=mean(fear))
my_geo$pais_avg <- pais_avg$x[match(my_geo$pais,pais_avg$pais)]
qplot(my_geo$fear,my_geo$pais_avg)
summary(lm(fear~pais_avg,data=my_geo))
```

So there's a correlation (we would hope!), but the R-squared value is only 0.011; this isn't the right level of aggregation.

Next, let's try `estratopri`, for regional divisions within each country.

```{r}
pri_avg <- ddply(my_geo,~estratopri,summarize,x=mean(fear))
my_geo$pri_avg <- pri_avg$x[match(my_geo$estratopri,pri_avg$estratopri)]
qplot(my_geo$fear,my_geo$pri_avg)
summary(lm(fear~pri_avg,data=my_geo))
```

So now R-squared has climbed to 0.11; we're accounting for 11% of the variance. Still not all that impressive. Let's move on to the province level:

```{r}
prov_avg <- ddply(my_geo,~prov,summarize,x=mean(fear))
my_geo$prov_avg <- prov_avg$x[match(my_geo$prov,prov_avg$prov)]
qplot(my_geo$fear,my_geo$prov_avg)
summary(lm(fear~prov_avg,data=my_geo))
```

With an R-squared of 0.13, provinces are barely better than regions. Now let's get really granular and look at municipalities. Are municipality IDs unique within each country?

```{r}
length(unique(my_geo$municipio)) # 149
length(unique(my_geo[my_geo$pais==2,'municipio'])) # 55
length(unique(my_geo[my_geo$pais==3,'municipio'])) # 52 
length(unique(my_geo[my_geo$pais==4,'municipio'])) # 51 -> 158 total
```

No. This means we'll need to create some kind of unique identifier that combines `pais` and `municipio`.

```{r}
my_geo$muni_uniq <- 10000*my_geo$pais + my_geo$municipio
muni_avg <- ddply(my_geo,~muni_uniq,summarize,x=mean(fear))
my_geo$muni_avg <- muni_avg$x[match(my_geo$muni_uniq,muni_avg$muni_uniq)]
qplot(my_geo$fear,my_geo$muni_avg)
summary(lm(fear~muni_avg,data=my_geo))
```

Even municipalities, the most granular level of geographic resolution we have across all three countries, only explain 22% of the variance in our fear index. Clearly, there's more determining people's level of fear than where they live. The plot is telling, though -- in the least fearful cities, hardly anyone has high a high fear index, while the opposite is true for the most fearful cities.

In Honduras, it's possible to get even more detail and go down to the district level.

```{r}
geo_hnd$distrito <- lapop.2014.HND$hondistrito
geo_hnd$fear <- my_geo[my_geo$pais==4,'fear']
dist_avg <- ddply(geo_hnd,~distrito,summarize,x=mean(fear))
geo_hnd$dist_avg <- dist_avg$x[match(geo_hnd$distrito,dist_avg$distrito)]
qplot(geo_hnd$fear,geo_hnd$dist_avg)
summary(lm(fear~dist_avg,data=geo_hnd))
```

In Honduras, then, the availability of district-level geolocation lets us explain 30% of the variance in fear.

How can I turn this into a better map of fear across the Northern Triangle? 