---
title: "Verify correlations"
author: "Craig"
date: "12 December 2015"
output: html_document
---

Rather than re-calculating all of the indices here, we've got the code to do this stashed in a separate R file.

``` {r}
source('make_indices.R')
```

To systematically explore all possible correlations with our fear index, we first need to see which haven't been accounted for in one of our other composite indices.

```{r}
n_fear_more <- c(fear_common,'vic40','vic41','vic43','vic45','fear6f')
n_ca_common <- c('cp5','cp7','cp8','cp13','cp20')
n_ca_more <- c(n_ca_common,'honcp22','honcp21a','cp21')
n_pv_gtm <- c('pv1','pv2a','pv2b','pv2c','pv2d','pv2e','pv2f',
            'pv2g','pv2h','pv2i','pv2j','pv2k')
n_ex_common <- c('exc2','exc6','exc20','exc11','exc13','exc14',
               'exc15','exc16','exc7')
n_tr_more <- c(tr_common,'pr4','m1','b11','esb48','epp1','epp3','pr4',
             'epn3a','epn3b','epn3c','b11','b37','b14','b15','b19',
             'b46','honb51','venb11','venhonb51',
             'venhonvb10','epp1','epp3','aoj18')
n_w_more <- c(w_common,'inf3a')
n_aut_hnd <- c('dem2','dem11','aut1','jc13','jc10','jc15a',
             'jc16a','honjc17')
n_aut_common <- c('dem2','dem11','jc13','jc10','jc15a')
n_geo_more <- c('pais','estratopri','estratosec','upm','prov','municipio',
              'cluster','tamano','hondistrito')
n_dont_use <- c('idnum','fecha','wt','uniq_id','nationality','vb3n','vb11',
              'leng1')

used <- unique(c(n_fear_more,n_ca_more,n_pv_gtm,n_ex_common,n_tr_more,
                 n_w_more,crit_common,n_aut_hnd,n_geo_more,n_dont_use))

unused_common <- names(lapop.2014.all)[!(names(lapop.2014.all) %in% used)]
unused_gtm <- names(lapop.2014.GTM)[!(names(lapop.2014.GTM) %in% used) & 
                                    !(names(lapop.2014.GTM) %in% unused_common)]
unused_slv <- names(lapop.2014.SLV)[!(names(lapop.2014.SLV) %in% used) & 
                                      !(names(lapop.2014.SLV) %in% unused_common)]
unused_hnd <- names(lapop.2014.HND)[!(names(lapop.2014.HND) %in% used) & 
                                      !(names(lapop.2014.HND) %in% unused_common)]
# common questions with country-specific answers
addback <- c('vb3n','vb11','leng1')
unused_gtm <- c(unused_gtm,addback)
unused_slv <- c(unused_slv,addback)
unused_hnd <- c(unused_hnd,addback)
```

We now have a list of unused variables that are common to all three countries, as well as a list of the unique unused variables for each country. Next, we need to categorize these variables as binary, ordered, or unordered categorical.

```{r}
# Assume a function is binary if only two responses are present
is_binary <- function(data,var) {
  u <- unique(data[,var])
  length(u[u<888888]) == 2
}

bin_common <- unused_common[sapply(unused_common,function(x) 
  is_binary(lapop.2014.all,x))]
bin_gtm <- unused_gtm[sapply(unused_gtm,function(x) 
  is_binary(lapop.2014.GTM,x))]
bin_slv <- unused_slv[sapply(unused_slv,function(x) 
  is_binary(lapop.2014.SLV,x))]
bin_hnd <- unused_hnd[sapply(unused_hnd,function(x) 
  is_binary(lapop.2014.HND,x))]

# Note that, while the indicators vb3n, vb11, and leng1 are used in all three
# countries, the answers are country-specific and shouldn't be used as 
# regional indicators.
unord_common <- c('idiomaq','a4','vic2','vic2aa','aoj22','env1','vb1',
                  'vb4new','vb101','vb20','for1n','for4',
                  'for5','q3c','ocup4a','ocup1a','q11n','etid')
ord_common <- unused_common[!(unused_common %in% bin_common) &
                            !(unused_common %in% unord_common)]
unord_gtm <- c('aoj21','chipart107n','parclien','guaetid2n','leng4','vb3n','vb11','leng1')
ord_gtm <- unused_gtm[!(unused_gtm %in% bin_gtm) &
                        !(unused_gtm %in% unord_gtm)]
unord_slv <- c('esexc16a','elsvb48','pr1','vb3n','vb11','leng1')
ord_slv <- unused_slv[!(unused_slv %in% bin_slv) &
                        !(unused_slv %in% unord_slv)]
unord_hnd <- c('dst1','vb3n','vb11','leng1')
ord_hnd <- unused_hnd[!(unused_hnd %in% bin_hnd) &
                              !(unused_hnd %in% unord_hnd)]
```

The appopriate method for finding correlations will be different for each of these three types of variables. I've defined three functions that take the same inputs and produce similar outputs, so that all can be combined and compared at the end.

Note that all of the functions below set the p-value cutoff, by default, to `1/ncol(data)` This is appropriate in cases (like this one) where we're looking at a large number of variables -- if we have more variables then we're more likely to observe spurious correlations so we need to be stricter. 

```{r}
bin_cor <- function(data,var1,var2,cutoff=0) {
  # Determine the correlation between a variable var1 and a binary variable 
  # var2. This uses a Fisher test conditioned on the values of var2, and will
  # be applicable if var1 is continuous (i.e. one of our composite indices).
  res = data.frame(var=character(),est=numeric(),pval=numeric(),
                   stringsAsFactors=FALSE)
  if (var1 != var2) {
    if (cutoff == 0) {
      cutoff <- 1 / ncol(data)
    }
    tmp <- data.frame(v1=data[,var1],v2=data[,var2])
    is.na(tmp[tmp>800000]) <- TRUE
    tmp$v2 <- tmp$v2 - min(tmp$v2,na.rm=TRUE) # convert to 0-1 scale
    if (sum(tmp$v2,na.rm=TRUE) > 1) {
      tt <- t.test(tmp$v1[tmp$v2==0],tmp$v1[tmp$v2==1])
      if (tt$p.value < cutoff) {
        res=data.frame(var=var2,est=tt$estimate[2]-tt$estimate[1],pval=tt$p.value,
                       stringsAsFactors=FALSE)
      }
    }
  }
  res
}

ord_cor <- function(data,var1,var2,cutoff=0) {
  # Determine the correlation between two ordered variables, var1 and var2. 
  # This uses linear regression, and will be applicable when we're dealing
  # with ordered categorical variables or composite indices.
  # As a matter of convention, use the composite index as var1 so that the
  # output makes sense.
  res = data.frame(var=character(),est=numeric(),pval=numeric(),
                   stringsAsFactors=FALSE)
  if (var1 != var2) {
    if (cutoff == 0) {
      cutoff <- 1 / ncol(data)
    }
    tmp <- data.frame(v1=data[,var1],v2=data[,var2])
    is.na(tmp[tmp>800000]) <- TRUE
    reg <- lm(v1 ~ v2,data=tmp)
    p <- summary(reg)$coefficients[2,4]
    if (p < cutoff) {
      res=data.frame(var=var2,est=summary(reg)$coefficients[2,1],pval=p,
                     stringsAsFactors=FALSE)
    }
  }
  res
}

unord_cor <- function(data,var1,var2,cutoff=0) {
  # Determine the correlation between an ordered variable var1 and an unordered
  # categorical variable var2. Do this by creating a binary variable for each
  # possible value of var2 and calling bin_cor().
  vals <- na.omit(unique(data[data[,var2] < 800000,var2]))
  if (cutoff == 0) {
    cutoff <- 1 / ncol(data)
  }
  tmp <- data.frame(v1=data[,var1])
  is.na(tmp[tmp>800000]) <- TRUE
  for (x in vals) {
    tmp[,paste(var2,x,sep='_')] <- as.numeric(data[,var2] == x)
  }
  ldply(names(tmp),function(x) bin_cor(tmp,'v1',x,cutoff))
}
```

**Region-wide**

Now we're going to look at the region-wide correlations. First, define a data frame that will contain all of the variables that were not used to generate composite indices, along with the composite indices.

```{r}
a <- lapop.2014.all[,unused_common]
is.na(a[a>800000]) <- TRUE
# add in all of the composite indices
a$fear_idx <- fear_all
a$ca_idx <- ca_all
a$tr_idx <- tr_all
a$w_idx <- w_all
a$crit_idx <- crit_all
a$aut_idx <- aut_all
idxs <- c('fear_idx','ca_idx','tr_idx','w_idx','crit_idx','aut_idx')
```

Next, we'll use the functions defined above to get all of our correlations. We'll keep those with p-values below the cutoff of `r 1/ncol(a)`

```{r}
cor_all_bin <- ldply(bin_common, function(x) bin_cor(a,'fear_idx',x))
cor_all_ord <- ldply(ord_common, function(x) ord_cor(a,'fear_idx',x))
cor_all_idx <- ldply(idxs, function(x) ord_cor(a,'fear_idx',x))
cor_all_unord <- ldply(unord_common, function(x) unord_cor(a,'fear_idx',x))
cor_all <- rbind(cor_all_bin,cor_all_ord,cor_all_unord,cor_all_idx)
cor_all <- cor_all[order(cor_all$pval),]
head(cor_all,15)
```

While we have a list of which values of unordered categorical variables are significantly correlated with the fear index, the variables aren't formatted in a way that will be useful for multiple regression. We'll do this by creating dummy variables, with the same names as the `var` column in `cor_all`. (For example, `ocup1a_4` for `ocup1a == 4`.)

```{r}
unord_vars <- ldply(cor_all_unord$var, function(x)
  data.frame(var=strsplit(x,'_')[[1]][1],
             val=as.numeric(strsplit(x,'_')[[1]][2]),
             str=x,
             stringsAsFactors=FALSE))
for (i in 1:nrow(unord_vars)) {
  a[,unord_vars$str[i]] <- as.numeric(a[,unord_vars$var[i]] == unord_vars$val[i])
}
a2 <- a[,cor_all$var]
a2$fear_idx <- a$fear_idx
```

One thing that can cause trouble in multiple regression is missing values -- we can only include rows in our dataframe that have a non-NA value for all of the variables compared. This means that as we add in more and more variables, we throw out more and more data, and our final results can be biased. We'll get around this by using multiple imputation. We'll need to create a predictor matrix (named `pm`) to account for the fact that we don't want to be using our fear index to impute values of the other variables -- this would be sort of circular logic!

```{r}
pm <- 1 - diag(ncol(a2))
pm[,which(names(a2)=='fear_idx')] <- 0
b <- mice(a2,printFlag=F,predictorMatrix=pm)
```

As our first test of significance, we'll throw everything into a multiple correlation, and get a list of the variables for which the p-value in the multiple correlation is less than `1/ncol(a)`.

```{r}
get_pvals <- function(i) {
  bi <- complete(b,i)
  pvals <- summary(lm(fear_idx ~ .,data=bi))$coefficients[,4]  
  pvals[names(pvals) != '(Intercept)']
}

pvals <- apply(ldply(1:5,get_pvals),2,max)
pvals[pvals<1/ncol(a)] # apppropriate cutoff = 1/n


```

Which values are significant?

- `ur`: Urbanization
- `it1`: Interpersonal trust
- `tr_idx`: Trust in government index
- `pole2n`: Satisfaction with police performance
- `aut_idx`: Authoritarianism index
- `pol1`: Interest in politics
- `clien1n`: Knows someone offered benefit for vote
- `mil10c`: Trustworthiness of Iranian government
- `ocup4a == 3`: Actively looking for a job
- `a4 == 5`: Believes crime is the most serious problem
- `q3c == 5`: Evangelical/Protestant
- `ca_idx`: Community activity index

Rather than throwing everything in and seeing which correlations are the strongest, we could also try going the other direction, gradually "pruning" correlations and stopping when only the strongest remain. Does this give us different results?

```{r}
prune <- function(imp,i,cutoff=0) {
  bi <- complete(imp,i)
  pvals <- summary(lm(fear_idx ~ .,data=bi))$coefficients[,4]
  pvals <- pvals[names(pvals) != '(Intercept)']
  if (cutoff == 0) {
    cutoff <- 1 / nrow(bi)
  }
  while (max(pvals > cutoff)) {
    bi <- bi[,-which(names(bi)==names(which.max(pvals)))]
    pvals <- summary(lm(fear_idx ~ .,data=bi))$coefficients[,4]
    pvals <- pvals[names(pvals) != '(Intercept)']
  }
  bi
}
Reduce(intersect,llply(1:5,function(i) names(prune(b,i,1/ncol(a)))))
```

This gives us a couple of new leads to follow:

- `for6`: Influence of China in country
- `aoj22 == 1`: Reduce crime by implementing preventive measures

Finally, it's worth asking which pairwise correlations are the strongest after multiple imputation. These could also give us some useful leads to follow in the social media analysis. Rather than keeping all below a certain cutoff, we'll look at the top 15 (the same number we got in the last step).

```{r}
# TODO: I can do this more sophisticatedly by using with.mids() and pooling, I think...
get_justone <- function(imp,idx,var) {
  pvals <- sapply(1:5,function(i) 
    summary(lm(idx ~ complete(imp,i)[,var]),data=complete(imp,i))$coefficients[2,4])
  max(pvals)
}
pair_pvals <- sort(sapply(names(a2),function(x) get_justone(b,fear_all,x)))
pair_pvals[2:16]
```

We see the usual suspects, plus:

- `www1`: Internet usage
- `ed`: Years of schooling
- `ed2`: Education level of mother
- `w_idx`: Wealth index
- `sd3new2`: Satisfaction with public schools
- `sd6new2`: Satisfaction with public health services
- `pn4`: Satisfaction with democracy
- `q14`: Intends to live or work abroad

To get average coefficients over the five imputed datasets, we'll need to pool results:

```{r}
b.reg <- with(data=b,exp=lm(fear_idx ~ ur + it1 + tr_idx + pole2n + aut_idx + 
                              for6 + pol1 + clien1n + mil10c + ocup4a_3 + a4_5 + 
                              q3c_5 + ca_idx + aoj22_1 + fear_idx))
b.reg.pool <- pool(b.reg)
summary(b.reg.pool)
```

`mil10c` is intriguing -- more fearful people consider the Iranian government more trustworthy. Does the idea of an authoritarian theocracy appeal to them? Also people interested in politics are less fearful, and people who see a large role for China in their country are more fearful.

We can also try getting to a decent set of variables using stepwise regression. What we'll do is run backward stepwise regression on each imputed dataset and keep only the ones that appear for all five sets.

```{r}
lm1 <- lm(fear_idx ~ .,data=na.omit(complete(b,1)))
lm2 <- lm(fear_idx ~ .,data=na.omit(complete(b,2)))
lm3 <- lm(fear_idx ~ .,data=na.omit(complete(b,3)))
lm4 <- lm(fear_idx ~ .,data=na.omit(complete(b,4)))
lm5 <- lm(fear_idx ~ .,data=na.omit(complete(b,5)))
library(MASS)
step1 <- stepAIC(lm1,direction="both",trace=F) # final AIC = -967.9723
step2 <- stepAIC(lm2,direction="both",trace=F) # final AIC = -946.0618
step3 <- stepAIC(lm3,direction="both",trace=F) # final AIC = -951.4168
step4 <- stepAIC(lm4,direction="both",trace=F) # final AIC = -951.9888
step5 <- stepAIC(lm5,direction="both",trace=F) # final AIC = -941.9365
counts <- as.data.frame(table(c(names(coef(step1)),names(coef(step2)),
                                names(coef(step3)),names(coef(step4)),
                                names(coef(step5)))),stringsAsFactors=F)
counts[counts$Freq==5,'Var1']
nrow(counts[counts$Freq==5,]) / nrow(counts) # 55% appear in all 5

b.reg2 <- with(data=b,exp=lm(fear_idx ~ a4_5 + aoj22_1 + aut_idx + ca_idx + 
                               clien1n + d6 + ed2 + for1n_7 + for4_4 + 
                               for5_6 + for6 + it1 + mil10c + ocup4a_3 + 
                               pol1 + pole2n + q3c_5 + sd3new2 + tr_idx + 
                               ur + vb20_4 + w14a + www1))
summary(b.reg2)
```

This returns all of the variables we found by looking at the first multiple correlation (`it1`, `ur`, `tr_idx`, `ca_idx`, `aut_idx`, `q3c_5`, `pole2n`, `pol1`, `a4_5`, `mil10c`, `clien1n`, `ocup4a_3`), as well as the two additional variables we got by the informal "pruning" procedure above (`for6`,`aoj22_1`). The correlation with `www1` is significant now; others have weaker significance. 

**Guatemala**

Next, let's see if any of the Guatemala-specific variables are interesting. The remainder will (as they say) be left as an exercise for the reader.

```{r}
g <- lapop.2014.GTM[,c(unused_common,unused_gtm)]
is.na(g[g>800000]) <- TRUE
g$fear_idx <- fear_gtm
g$ca_idx <- ca_all[lapop.2014.all$pais==2] # there is no ca_gtm
g$tr_idx <- tr_gtm
g$w_idx <- w_gtm
g$crit_idx <- crit_all[lapop.2014.all$pais==2]
g$aut_idx <- aut_all[lapop.2014.all$pais==2]
g$pv_idx <- pv_gtm
idxs <- c('fear_idx','ca_idx','tr_idx','w_idx','crit_idx','aut_idx','pv_idx')

cor_gtm_bin <- ldply(c(bin_common,bin_gtm), function(x) bin_cor(g,'fear_idx',x))
cor_gtm_ord <- ldply(c(ord_common,ord_gtm), function(x) ord_cor(g,'fear_idx',x))
cor_gtm_idx <- ldply(idxs, function(x) ord_cor(g,'fear_idx',x))
cor_gtm_unord <- ldply(c(unord_common,unord_gtm), function(x) 
  unord_cor(g,'fear_idx',x))
cor_gtm <- rbind(cor_gtm_bin,cor_gtm_ord,cor_gtm_unord,cor_gtm_idx)
cor_gtm <- cor_gtm[order(cor_gtm$pval),]
head(cor_gtm,15)
```

We definitely see a few variables that weren't on the menu before; some of these we might want to add into the indices we have.

```{r}
unord_vars_g <- ldply(cor_gtm_unord$var, function(x)
  data.frame(var=strsplit(x,'_')[[1]][1],
             val=as.numeric(strsplit(x,'_')[[1]][2]),
             str=x,
             stringsAsFactors=FALSE))
for (i in 1:nrow(unord_vars_g)) {
  g[,unord_vars_g$str[i]] <- as.numeric(g[,unord_vars_g$var[i]] == unord_vars_g$val[i])
}
g2 <- g[,cor_gtm$var]
g2$fear_idx <- g$fear_idx

pm_g <- 1 - diag(ncol(g2))
pm_g[,which(names(g2)=='fear_idx')] <- 0
g_imp <- mice(g2,printFlag=F,predictorMatrix=pm_g)
```

Let's cut right to the chase and use stepwise regression on our multiply-imputed data:

```{r}
lm1g <- lm(fear_idx ~ .,data=na.omit(complete(g_imp,1)))
lm2g <- lm(fear_idx ~ .,data=na.omit(complete(g_imp,2)))
lm3g <- lm(fear_idx ~ .,data=na.omit(complete(g_imp,3)))
lm4g <- lm(fear_idx ~ .,data=na.omit(complete(g_imp,4)))
lm5g <- lm(fear_idx ~ .,data=na.omit(complete(g_imp,5)))
step1g <- stepAIC(lm1g,direction="both",trace=F) # final AIC = -1016.8753
step2g <- stepAIC(lm2g,direction="both",trace=F) # final AIC = -997.2276
step3g <- stepAIC(lm3g,direction="both",trace=F) # final AIC = -1014.1022
step4g <- stepAIC(lm4g,direction="both",trace=F) # final AIC = -1005.7721
step5g <- stepAIC(lm5g,direction="both",trace=F) # final AIC = -993.3492
counts_g <- as.data.frame(table(c(names(coef(step1g)),names(coef(step2g)),
                                names(coef(step3g)),names(coef(step4g)),
                                names(coef(step5g)))),stringsAsFactors=F)
counts_g[counts_g$Freq==5,'Var1']
nrow(counts[counts_g$Freq==5,]) / nrow(counts_g) # 55% appear in all 5

g.reg <- with(data=g_imp,exp=lm(fear_idx ~ a4_27 + a4_5 + aoj21_1 + aoj21_2 + 
                                  aoj21_8 + aut_idx + d6 + fear6e + for4_1 + 
                                  guaetid2n_1 + guaetid2n_16 + it1 + leng4_1 + 
                                  per9 + pn4 + pole2n + pv_idx + pv3 + q3c_2 + 
                                  tr_idx + ur + vb20_3 + vb20_4 + w_idx))
summary(g.reg)
```

So what are the strongest correlates in Guatemala?

- `fear6e`: Insecurity on public transportation (add to index?)
- `aoj21 == 2`: Gangs are biggest threat to security
- `ur`: Urbanizaton
- `aoj21 == 8`: Biggest threat = none of the above (much less fearful)
- `it1`: Interpersonal trust
- `aut_idx`: Authoritarianism index
- `pv_idx`: Political violence index
- `leng4 == 1`: Parents' language (Spanish vs. indigenous)
- `aoj21 == 8`: Biggest threat = your neighbors (less fearful)
- `tr_idx`: Trust in government index
- `vb20 == 4`: Plans to vote but leave ballot blank
- `a4 == 5`: Crime the biggest problem
- `guaetid2n == 16`: Ethnic group = Q'eqchi'
- `a4 == 27`: Most serious problem = lack of security
- `pv3`: Believes violence was used in 2015 elections
- `d6`: Approval of same-sex marriage (higher = more fear)
- `pole2n`: Satisfaction with police performance
- `guaetid2n == 1`: Ethnic group = Achí
- `pn4`: Satisfaction with democracy

We see some new themes, as well as some overlap with the region-wide trends.

**El Salvador**

**Honduras**

